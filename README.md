# UK News Scraper + N8N Digest

> Automated UK news digest delivered to Gmail twice daily — built with Python, Docker, and N8N.

This project is a training tool for anyone learning **Python, Claude Code, and N8N** in the context of AI automation. The scraper collects top UK news from four sources and emails a styled HTML digest every morning and evening.

The majority of the content was generated by **Claude Code**. The intention is to foster further learning using Claude Code, Python, and N8N to solve real-life problems.

---

## Docker Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│  HOST MACHINE  (Ubuntu Linux)                                           │
│                                                                         │
│  ┌──────────────────────────────────────────────────────────────────┐   │
│  │  DOCKER NETWORK                                                  │   │
│  │                                                                  │   │
│  │   ┌─────────────────────┐   HTTP GET /run   ┌─────────────────┐ │   │
│  │   │   n8n  container    │ ────────────────► │ scraper         │ │   │
│  │   │                     │ ◄──────────────── │ container       │ │   │
│  │   │  docker.n8n.io/n8n  │   JSON response   │ python:3.12-slim│ │   │
│  │   │  port 5678          │                   │ port 8765       │ │   │
│  │   │                     │                   │                 │ │   │
│  │   │  Schedule: 7AM/6PM  │                   │ GET /run        │ │   │
│  │   │  Gmail OAuth2       │                   │ GET /health ✅  │ │   │
│  │   │  n8n_data volume    │                   │ output/ volume  │ │   │
│  │   └─────────────────────┘                   └─────────────────┘ │   │
│  │          depends_on: scraper (healthy)                           │   │
│  └──────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  Browser → localhost:5678 (N8N UI)    curl → localhost:8765/health      │
└─────────────────────────────────────────────────────────────────────────┘
```

### End-to-End Data Flow

```
① Cron fires     → ② N8N HTTP GET /run   → ③ Scraper fetches articles
        ↓                                          (BBC · Guardian · Ind · Sky)
⑧ Email received ← ⑦ Gmail OAuth2 send  ← ⑥ JSON returned to N8N
                                          ← ⑤ HTML digest saved to disk
                                          ← ④ Top 10 per source selected
```

---

## scraper.py — Code Map

```
scraper.py
│
├── Imports ──────────────── csv, json, os, requests, feedparser, BeautifulSoup
│
├── Configuration ────────── USER_AGENT, REQUEST_TIMEOUT, MIN/MAX_DELAY, paths
│
├── Article (dataclass) ──── source · title · url · summary · author · date
│
├── Utility functions
│   ├── polite_get(url)  ──── safe HTTP GET with timeout + error handling
│   ├── parse_date(raw)  ──── normalise any date string → ISO 8601
│   ├── delay()          ──── random 2–5s polite pause between requests
│   ├── parse_feed(url)  ──── fetch and parse RSS/Atom feed
│   └── strip_html(raw)  ──── remove HTML tags, return plain text
│
├── Per-source scrapers
│   ├── scrape_bbc()         RSS + per-article HTML for author
│   ├── scrape_guardian()    Guardian Content JSON API
│   ├── scrape_independent() RSS + HTML author fallback
│   └── scrape_sky_news()    RSS + HTML author fallback
│
├── Output writers
│   ├── save_to_csv()        spreadsheet format
│   ├── save_to_html()       styled HTML table
│   ├── save_to_json()       JSON with metadata envelope
│   └── save_to_odt()        LibreOffice Writer document
│
├── Email builder
│   └── get_html_email_body() Gmail-safe inline CSS · top 10 per source · 700px
│
└── main() ───────────────── runs all scrapers → saves all output formats
```

---

## CI/CD Pipeline

```
develop branch  ──► lint → test → build → push :dev   → development env
test branch     ──► lint → test → build → push :test  → test env
master branch   ──► lint → test → build → push :latest → production env

git tag v1.2.3  ──► versioned build → SSH deploy → healthcheck
```

| Job | Trigger | Result |
|-----|---------|--------|
| Lint (flake8) | all branches | style check |
| Unit Tests (32) | all branches | pytest + coverage.xml |
| Docker Build | all branches | validates Dockerfile.scraper |
| Deploy to Development | `develop` push | `:dev` image → GHCR |
| Deploy to Test | `test` push | `:test` image → GHCR |
| Deploy to Production | `master` push | `:latest` image → GHCR |

---

## News Sources

| Source | Method | Author |
|--------|--------|--------|
| BBC News | RSS + per-article HTML | CSS selector scrape |
| The Guardian | Content JSON API | API `byline` field |
| The Independent | RSS + HTML fallback | `meta[name=author]` |
| Sky News | RSS + HTML fallback | CSS selector scrape |

---

## Quick Start

```bash
# 1. Clone
git clone https://github.com/sufideen/uk-news-scraper.git
cd uk-news-scraper

# 2. Python standalone
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt
python3 scraper.py
# Output: output/news_articles.csv · .html · .json · .odt

# 3. Docker + N8N (full stack)
cd /home/sufideen
docker compose up -d
# N8N UI → http://localhost:5678
# Import n8n_workflow.json → assign Gmail credential → Activate
```

---

## Project Files

| File | Purpose |
|------|---------|
| `scraper.py` | Main scraper — all 4 sources, 4 output formats |
| `scraper_server.py` | HTTP sidecar server (`GET /run`, `GET /health`) |
| `run_for_n8n.py` | CLI wrapper — outputs JSON envelope to stdout |
| `Dockerfile.scraper` | Python 3.12-slim container for scraper sidecar |
| `n8n_workflow.json` | Importable N8N workflow (Schedule → HTTP → Gmail) |
| `requirements.txt` | Python dependencies |
| `TRAINING_GUIDE.md` | Beginner Python training document (10 sections) |
| `tests/test_scraper.py` | 32 unit tests (pytest) |
| `.github/workflows/ci.yml` | CI pipeline — lint, test, build, deploy |
| `.github/workflows/cd.yml` | CD pipeline — release tags → server deploy |
| `visuals_scraper_map.html` | Interactive scraper code map |
| `visuals_docker_architecture.html` | Interactive Docker architecture diagram |

---

## Training Resources

- **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)** — Full beginner Python walkthrough with exercises, glossary and error reference
- **[visuals_scraper_map.html](visuals_scraper_map.html)** — Interactive scraper.py section map
- **[visuals_docker_architecture.html](visuals_docker_architecture.html)** — Interactive Docker architecture diagram

---

## Requirements

- Python 3.12+
- Docker + Docker Compose
- N8N (self-hosted or cloud)
- Gmail OAuth2 credential (Google Cloud Console)

---

*Generated with [Claude Code](https://claude.ai/claude-code) · BBC News · The Guardian · The Independent · Sky News*
